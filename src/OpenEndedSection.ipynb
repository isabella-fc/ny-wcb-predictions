{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: darkslategray; color: white; padding: 15px; border-radius: 8px;\">\n",
    "    <center><h1 style=\"font-family: Arial, sans-serif;\">TO GRANT OR NOT TO GRANT: DECIDING ON COMPENSATION BENEFITS</h1></center>\n",
    "    <center><h3 style=\"font-family: Arial, sans-serif;\">Machine Learning Project</h3></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: white; padding: 15px; border-radius: 8px;\">\n",
    "    <center><a href=\"https://github.com/isabella-fc/to-grant-or-not-to-grant/tree/main/ml_project/wcb\" target=\"_blank\" style=\"font-size: 26px; color: #0000FF; text-decoration: none;\">Github Repository (Web Application)</a></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Ended Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is divided into two parts.<br>\n",
    "The first is a short demonstration of the process of transforming the model and features in Â´to an applicable form for an web application (the whole process and files are accesible through the link to the github repository)<br>\n",
    "The second is an assesment of models and creation of a final model to predict the \"Agreement Reached\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Sklearn: Model Selection\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, PredefinedSplit, StratifiedKFold, train_test_split, cross_val_score\n",
    ")\n",
    "\n",
    "# Sklearn: Feature Selection\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Sklearn: Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Sklearn: Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, classification_report, make_scorer\n",
    ")\n",
    "\n",
    "\n",
    "# Custom Preprocessing Functions\n",
    "from Preprocessing_functions import *\n",
    "\n",
    "# Reload Preprocessing Functions (for updates during runtime)\n",
    "imported_module = importlib.import_module(\"Preprocessing_functions\")\n",
    "importlib.reload(imported_module)\n",
    "\n",
    "# Pandas Display Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Plotting Settings\n",
    "sns.set()  # Apply Seaborn style globally for plots\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)  # Set default plot size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_form(form_data):\n",
    "    \"\"\"\n",
    "    Preprocess form.cleaned_data for XGBoost prediction:\n",
    "    - Converts all string inputs into appropriate numeric types.\n",
    "    - Converts categorical variables into numerical values.\n",
    "    - Ensures strict column order according to FEATURE_ORDER.\n",
    "    \"\"\"\n",
    "    # Helper functions\n",
    "    def parse_boolean(value):\n",
    "        return 1 if str(value).lower() == 'true' else 0 if str(value).lower() == 'false' else np.nan\n",
    "\n",
    "    # Initialize processed_data with default values (0 or np.nan)\n",
    "    processed_data = {feature: 0 for feature in FEATURE_ORDER}\n",
    "\n",
    "    # Numeric fields\n",
    "    processed_data['Age at Injury'] = form_data.get('age_at_injury', np.nan)\n",
    "    processed_data['Average Weekly Wage'] = float(form_data.get('average_weekly_wage', np.nan))\n",
    "    processed_data['Birth Year'] = form_data.get('birth_year', np.nan)\n",
    "    ime4count = form_data.get('ime4_count', 0)\n",
    "    processed_data['IME-4 Count'] = ime4count != 0\n",
    "    processed_data['Number of Dependents'] = form_data.get('number_of_dependents', 0)\n",
    "\n",
    "    # Date fields\n",
    "    for field, prefix in {\n",
    "        'accident_date': 'Accident Date_',\n",
    "        'c2_date': 'C-2 Date_',\n",
    "        'assembly_date': 'Assembly Date_',\n",
    "    }.items():\n",
    "        date_value = form_data.get(field, None)\n",
    "        print(f\"Processing date field: {field}, Value: {date_value}, Type: {type(date_value)}\")  # Debugging\n",
    "        processed_data = add_datetime_features(processed_data, date_value, prefix)\n",
    "\n",
    "    # Boolean fields\n",
    "    boolean_fields = {\n",
    "    'covid_indicator': 'COVID-19 Indicator',\n",
    "    'c3_form_submitted': 'Has C-3 Date',\n",
    "    'first_hearing_date': 'Has First Hearing Date',\n",
    "}\n",
    "\n",
    "    # Process boolean fields (idk why)\n",
    "    for key, prefix in boolean_fields.items():\n",
    "        field_value = form_data.get(key)\n",
    "        processed_data = add_boolean_one_hot(processed_data, field_value, prefix)\n",
    "\n",
    "\n",
    "    # Carrier Type mapping\n",
    "    carrier_types = [\n",
    "        '1A. PRIVATE', '2A. SIF', '3A. SELF PUBLIC', '4A. SELF PRIVATE',\n",
    "        '5A. SPECIAL FUND - CONS. COMM. (SECT. 25-A)', 'UNKNOWN'\n",
    "    ]\n",
    "    carrier = form_data.get('carrier_type', 'UNKNOWN')\n",
    "    for ct in carrier_types:\n",
    "        processed_data[f'Carrier Type_{ct}'] = int(carrier == ct)\n",
    "\n",
    "    \n",
    "    # Gender mapping\n",
    "    gender_mapping = ['F', 'M', 'U', 'X']\n",
    "    gender = form_data.get('gender', 'U')\n",
    "    for g in gender_mapping:\n",
    "        processed_data[f'Gender_{g}'] = int(gender == g)\n",
    "\n",
    "    #Alternative dispute resolution mapping\n",
    "    adr_options ={'False': 'N', 'True': 'Y', 'None': 'U'}\n",
    "    for key, value in adr_options.items():\n",
    "        processed_data[f'Alternative Dispute Resolution_{value}'] = int(form_data.get('alternative_dispute_resolution', 'U') == key)\n",
    "\n",
    "    # representative and attorney mapping\n",
    "    attorney_options ={'False': 'N', 'True': 'Y',}\n",
    "    for key, value in attorney_options.items():\n",
    "        processed_data[f'Attorney/Representative_{value}'] = int(form_data.get('attorney_representative', 'False') == key)\n",
    "\n",
    "    #covid 19 indicator \n",
    "    covid_options ={'False': 'N', 'True': 'Y',}\n",
    "    for key, value in covid_options.items():\n",
    "        processed_data[f'COVID-19 Indicator_{value}'] = int(form_data.get('covid_indicator', 'False') == key)\n",
    "\n",
    "    medical_options = ['Medical Fee Region_I', \"Medical Fee Region_II\", \"Medical Fee Region_III\", \"Medical Fee Region_IV\"\"Medical Fee Region_UK\"]\n",
    "    for option in medical_options:\n",
    "        processed_data[option] = int(form_data.get('medical_fee_region', 'Medical Fee Region_UK') == option)\n",
    "\n",
    "    # County mapping\n",
    "    processed_data = match_one_hot_encoding(\n",
    "        processed_data, COUNTIES, form_data.get('county_of_injury', 'UNKNOWN'), 'County of Injury_'\n",
    "    )\n",
    "\n",
    "    processed_data = match_one_hot_encoding(\n",
    "        processed_data, INDUSTRY_CODES, form_data.get('industry_code', 'UNKNOWN'), 'Industry Code_', float_conversion=True\n",
    "    )\n",
    "\n",
    "    processed_data = match_one_hot_encoding(\n",
    "        processed_data, CAUSE_OF_INJURY_CODES, form_data.get('wcio_cause_of_injury_code', 'UNKNOWN'), 'WCIO Cause of Injury Code_', float_conversion=True\n",
    "    )\n",
    "\n",
    "    processed_data = match_one_hot_encoding(\n",
    "        processed_data, NATURE_OF_INJURY_CODES, form_data.get('wcio_nature_of_injury_code', 'UNKNOWN'), 'WCIO Nature of Injury Code_', float_conversion=True\n",
    "    )\n",
    "\n",
    "    processed_data = match_one_hot_encoding(\n",
    "        processed_data, PART_OF_BODY_CODES, form_data.get('wcio_part_of_body_code', 'UNKNOWN'), 'WCIO Part Of Body Code_', float_conversion=True\n",
    "    )\n",
    "\n",
    "    district_options = ['District Name_ALBANY', 'District Name_BINGHAMTON', 'District Name_BUFFALO','District Name_HAUPPAUGE','District Name_NYC','District Name_ROCHESTER','District Name_STATEWIDE','District Name_SYRACUSE',]\n",
    "    processed_data = match_one_hot_encoding(processed_data, district_options, form_data.get('district_name', 'UNKNOWN'), 'District Name_')\n",
    "\n",
    "    processed_data['Carrier Name'] = float(form_data.get('encoded_value_carrier', 0))\n",
    "    processed_data['Zip Code'] = float(form_data.get('encoded_value', 0))\n",
    "\n",
    "    # Create DataFrame and strictly enforce FEATURE_ORDER\n",
    "    processed_df = pd.DataFrame([processed_data])\n",
    "\n",
    "    # Drop unexpected columns\n",
    "    processed_df = processed_df[[col for col in FEATURE_ORDER if col in processed_df.columns]]\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def match_one_hot_encoding(df, features, value, prefix, float_conversion=False):\n",
    "    \"\"\"\n",
    "    Matches the first value in a one-hot encoded feature set\n",
    "    Leaves the rest 0\n",
    "    \"\"\"\n",
    "    if float_conversion:\n",
    "        value = float(value)\n",
    "\n",
    "    for feature in features:\n",
    "        # Check if the feature starts with the given prefix\n",
    "        if feature.startswith(prefix) :\n",
    "            # Set the column to 1 if it matches the target value, otherwise 0\n",
    "            df[feature] = 1 if feature == f\"{prefix}{value}\" else 0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_datetime_features(df, date_value, prefix):\n",
    "    \"\"\"\n",
    "    Adds year, month, day, and day of the week features for a given date to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (dict): The dictionary to update with new features.\n",
    "        date_value (datetime.date or str): The date value to extract features from.\n",
    "        prefix (str): The prefix for the new feature names.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated dictionary with the new date-related features.\n",
    "    \"\"\"\n",
    "    # Convert the date_value to pandas datetime if it's not None\n",
    "    date_value = pd.to_datetime(date_value, errors='coerce')  # Handles None and invalid dates\n",
    "    if pd.notnull(date_value):\n",
    "        df[f'{prefix}Year'] = date_value.year\n",
    "        df[f'{prefix}Month'] = date_value.month\n",
    "        df[f'{prefix}Day'] = date_value.day\n",
    "        df[f'{prefix}DayOfWeek'] = date_value.dayofweek\n",
    "    else:\n",
    "        # Assign NaN if the date is missing or invalid\n",
    "        df[f'{prefix}Year'] = np.nan\n",
    "        df[f'{prefix}Month'] = np.nan\n",
    "        df[f'{prefix}Day'] = np.nan\n",
    "        df[f'{prefix}DayOfWeek'] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_boolean_one_hot(df, field_value, prefix):\n",
    "    \"\"\"\n",
    "    Adds one-hot encoded columns for a boolean field to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (dict): The dictionary to update with new features.\n",
    "        field_value (str or bool): The field value ('True', 'False', or equivalent).\n",
    "        prefix (str): The prefix for the new one-hot encoded column names.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated dictionary with one-hot encoded columns.\n",
    "    \"\"\"\n",
    "    # Parse the field value to determine boolean state\n",
    "    parsed_value = 1 if str(field_value).lower() == 'true' else 0\n",
    "    \n",
    "    # Add one-hot encoded columns\n",
    "    df[f'{prefix}_1'] = parsed_value\n",
    "    df[f'{prefix}_0'] = 1 - parsed_value\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Agreement Reached Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timst\\AppData\\Local\\Temp\\ipykernel_39384\\3470921380.py:1: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv('train_data.csv', index_col='Claim Identifier')\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_data.csv', index_col='Claim Identifier')\n",
    "test_data = pd.read_csv('test_data.csv', index_col='Claim Identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[~(train_data.drop(columns=['Assembly Date']).isna().all(axis=1) & train_data['Assembly Date'].notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns=['Claim Injury Type', 'WCB Decision', 'Agreement Reached','OIICS Nature of Injury Description'])\n",
    "y = train_data['Agreement Reached']\n",
    "\n",
    "test_data = test_data.drop(columns=['OIICS Nature of Injury Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agreement Reached\n",
       "0.0    95.333487\n",
       "1.0     4.666513\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in percent\n",
    "y.value_counts()/len(y)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_COLUMNS = ['Industry Code', 'WCIO Cause of Injury Code',\n",
    "       'WCIO Nature of Injury Code', 'WCIO Part Of Body Code']\n",
    "\n",
    "DESCRIPTION_COLUMNS = ['WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description','Industry Code Description']\n",
    "\n",
    "BOOLEAN_COLUMNS = ['Alternative Dispute Resolution', 'Attorney/Representative','COVID-19 Indicator']\n",
    "\n",
    "date_order = ['Accident Date', 'C-2 Date','C-3 Date','Assembly Date', 'First Hearing Date']\n",
    "\n",
    "numerical_columns = [\n",
    "    'Accident Date', \n",
    "    'Age at Injury', \n",
    "    'Assembly Date', \n",
    "    'Average Weekly Wage', \n",
    "    'Birth Year', \n",
    "    'C-2 Date', \n",
    "    'C-3 Date', \n",
    "    'First Hearing Date', \n",
    "    'IME-4 Count', \n",
    "]\n",
    "\n",
    "outliers_columns = [\n",
    "    'Accident Date', \n",
    "    'Age at Injury', \n",
    "    'Assembly Date', \n",
    "    'Average Weekly Wage', \n",
    "    'Birth Year',\n",
    "    'IME-4 Count', \n",
    "]\n",
    "\n",
    "categorical_features = ['Alternative Dispute Resolution',\n",
    " 'Attorney/Representative',\n",
    " 'Carrier Name',\n",
    " 'Carrier Type',\n",
    " 'County of Injury',\n",
    " 'COVID-19 Indicator',\n",
    " 'District Name',\n",
    " 'Gender',\n",
    " 'Industry Code',\n",
    " 'Medical Fee Region',\n",
    " 'WCIO Cause of Injury Code',\n",
    " 'WCIO Nature of Injury Code',\n",
    " 'WCIO Part Of Body Code',\n",
    " 'Zip Code']\n",
    "\n",
    "\n",
    "columns_to_scale = ['Accident Date',\n",
    "                'Assembly Date',\n",
    "                'Average Weekly Wage',\n",
    "                'Age at Injury',\n",
    "                'Birth Year', \n",
    "                'Number of Dependents',\n",
    "                'IME-4 Count']\n",
    "\n",
    "date_columns = ['Accident Date', 'Assembly Date']\n",
    "\n",
    "outliers_iqr_specific = ['Age at Injury', 'Birth Year']\n",
    "\n",
    "columns_to_drop = ['C-2 Date', 'C-3 Date', 'First Hearing Date']\n",
    "\n",
    "low_cardinality_cols = [col for col in categorical_features if X[col].nunique() < 10]\n",
    "high_cardinality_cols = [col for col in categorical_features if X[col].nunique() > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_scaling_encoding_dum(X_train, X_val):\n",
    "    X_train, X_val = type_conversion_categorical(X_train, X_val,categorical_features)\n",
    "    X_train, X_val = drop_description_columns(X_train, X_val)\n",
    "    X_train, X_val = convert_to_timestamp(X_train, X_val, date_order)\n",
    "    X_train, X_val = convert_to_bool(X_train, X_val, col_names=BOOLEAN_COLUMNS)\n",
    "    X_train, X_val = impute_mean_numerical(X_train, X_val, numerical_columns)\n",
    "    X_train, X_val = fill_missing_with_mode(X_train, X_val)\n",
    "    X_train, X_val = feature_creation_has_Cdate(X_train, X_val)\n",
    "    X_train, X_val = drop_unwanted_columns(X_train, X_val, columns_to_drop)\n",
    "    X_train, X_val = scaling_robust(X_train, X_val, columns_to_scale)\n",
    "    X_train, X_val = encoding_onehot(X_train, X_val, low_cardinality_cols)\n",
    "    X_train, X_val = encoding_frequency1(X_train, X_val, high_cardinality_cols)\n",
    "\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predifined_split(X, y, preprocess_steps, n_splits = 5):\n",
    "    \"\"\"\n",
    "    Creates a PredefinedSplit object to be used in cross-validation, more specifically in GridSearchCV.\n",
    "\n",
    "    Steps:\n",
    "    - Defines the number of splits\n",
    "    - Splits the data into training and validation sets\n",
    "    - Applies the preprocessing steps to the training and validation sets\n",
    "    - Returns the PredefinedSplit object and the preprocessed data\n",
    "    \"\"\"\n",
    "\n",
    "    X_combined_list = []\n",
    "    y_combined_list = []\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    test_data = np.zeros(len(X), dtype=int) - 1\n",
    "\n",
    "    for fold_idx, (_, test_idx) in enumerate(kf.split(X, y)):\n",
    "        test_data[test_idx] = fold_idx\n",
    "\n",
    "    ps = PredefinedSplit(test_fold=test_data)\n",
    "\n",
    "    for train_index, test_index in ps.split():\n",
    "\n",
    "        # Get fold\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Preprocess and encode data    \n",
    "        X_train, X_val = preprocess_steps(X_train, X_val)\n",
    "        y_train, y_val, le = encoding_label(y_train, y_val)\n",
    "\n",
    "        X_combined_list.append(X_train)\n",
    "        y_combined_list.append(y_train)\n",
    "\n",
    "    X_combined = pd.concat(X_combined_list, axis=0)\n",
    "    y_combined = np.concatenate(y_combined_list, axis=0)\n",
    "\n",
    "    return ps, X_combined, y_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[coulmns] = X_train[coulmns].astype(str)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[coulmns] = X_val[coulmns].astype(str)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:456: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(mean_value, inplace=True)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:457: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val[col].fillna(mean_value, inplace=True)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[coulmns] = X_train[coulmns].astype(str)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[coulmns] = X_val[coulmns].astype(str)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:456: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(mean_value, inplace=True)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:457: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val[col].fillna(mean_value, inplace=True)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[coulmns] = X_train[coulmns].astype(str)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[coulmns] = X_val[coulmns].astype(str)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:456: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(mean_value, inplace=True)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:457: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val[col].fillna(mean_value, inplace=True)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[coulmns] = X_train[coulmns].astype(str)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[coulmns] = X_val[coulmns].astype(str)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:456: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(mean_value, inplace=True)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:457: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val[col].fillna(mean_value, inplace=True)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[coulmns] = X_train[coulmns].astype(str)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[coulmns] = X_val[coulmns].astype(str)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:456: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(mean_value, inplace=True)\n",
      "c:\\Users\\timst\\OneDrive\\Desktop\\NOVA IMS\\Semester 1\\MachineLearning\\Project\\ML_Group36\\src\\Preprocessing_functions.py:457: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val[col].fillna(mean_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "ps, X_combined, y_combined = create_predifined_split(X, y, preprocessing_scaling_encoding_dum, n_splits=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection (RFECV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,            # Number of trees\n",
    "    max_depth=15,                # Limit tree depth\n",
    "    min_samples_split=50,        # Minimum samples for a split\n",
    "    min_samples_leaf=20,         # Minimum samples per leaf\n",
    "    max_features='sqrt',         # Features to consider per split\n",
    "    class_weight='balanced',     # Handle class imbalance\n",
    "    bootstrap=True,              # Use bootstrapping\n",
    "    random_state=42,             # Ensure reproducibility\n",
    "    n_jobs=-1                    # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Set up RFECV with RandomForest and cross-validation\n",
    "rfecv = RFECV(estimator=rf_model, step=1, cv=ps, scoring='f1_macro') \n",
    "\n",
    "# Fit RFECV\n",
    "rfecv.fit(X_combined, y_combined)\n",
    "\n",
    "#Get the selected features\n",
    "selected_features_RFE_basic = X_combined.columns[rfecv.support_].tolist()\n",
    "optimal_num_features = rfecv.n_features_\n",
    "feature_ranking = rfecv.ranking_\n",
    "\n",
    "print(\"Optimal number of features:\", optimal_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_RFE_basic = ['Accident Date', 'Age at Injury', 'Assembly Date', 'Average Weekly Wage', 'Birth Year', 'IME-4 Count', 'Number of Dependents', 'Attorney/Representative_False', 'Attorney/Representative_True', 'Carrier Type_1A. PRIVATE', 'Carrier Type_2A. SIF', 'Carrier Type_3A. SELF PUBLIC', 'COVID-19 Indicator_False', 'COVID-19 Indicator_True', 'District Name_NYC', 'Gender_F', 'Gender_M', 'Medical Fee Region_I', 'Medical Fee Region_IV', 'Carrier Name', 'County of Injury', 'Industry Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code', 'Zip Code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_with_overfitting_check(X, y, predefined_split, selected_features, models):\n",
    "    \"\"\"\n",
    "    Evaluates multiple models on a dataset with a predefined split and calculates train-test metric differences.\n",
    "    \n",
    "    Parameters:\n",
    "    - X (pd.DataFrame): Feature dataset.\n",
    "    - y (pd.Series or np.array): Target variable.\n",
    "    - predefined_split (PredefinedSplit): Predefined split object for cross-validation.\n",
    "    - models (dict): Dictionary of models, where keys are model names and values are model instances.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Summary table with mean, variance, and train-test differences for evaluation metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        fold_metrics = {\n",
    "            'accuracy_train': [],\n",
    "            'accuracy_test': [],\n",
    "            'precision_macro_train': [],\n",
    "            'precision_macro_test': [],\n",
    "            'recall_macro_train': [],\n",
    "            'recall_macro_test': [],\n",
    "            'f1_macro_train': [],\n",
    "            'f1_macro_test': []\n",
    "        }\n",
    "        \n",
    "        # Loop through predefined splits\n",
    "        for train_idx, test_idx in predefined_split.split():\n",
    "            # Split data\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train[selected_features], y_train)\n",
    "            \n",
    "            # Predict on train and test data\n",
    "            y_train_pred = model.predict(X_train[selected_features])\n",
    "            y_test_pred = model.predict(X_test[selected_features])\n",
    "            \n",
    "            # Calculate metrics for train and test data\n",
    "            fold_metrics['accuracy_train'].append(accuracy_score(y_train, y_train_pred))\n",
    "            fold_metrics['accuracy_test'].append(accuracy_score(y_test, y_test_pred))\n",
    "            fold_metrics['precision_macro_train'].append(precision_score(y_train, y_train_pred, average='macro', zero_division=0))\n",
    "            fold_metrics['precision_macro_test'].append(precision_score(y_test, y_test_pred, average='macro', zero_division=0))\n",
    "            fold_metrics['recall_macro_train'].append(recall_score(y_train, y_train_pred, average='macro', zero_division=0))\n",
    "            fold_metrics['recall_macro_test'].append(recall_score(y_test, y_test_pred, average='macro', zero_division=0))\n",
    "            fold_metrics['f1_macro_train'].append(f1_score(y_train, y_train_pred, average='macro', zero_division=0))\n",
    "            fold_metrics['f1_macro_test'].append(f1_score(y_test, y_test_pred, average='macro', zero_division=0))\n",
    "        \n",
    "        # Calculate mean, variance, and train-test differences for each metric\n",
    "        for metric_name in ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']:\n",
    "            train_metric = np.array(fold_metrics[f'{metric_name}_train'])\n",
    "            test_metric = np.array(fold_metrics[f'{metric_name}_test'])\n",
    "            \n",
    "            mean_train = np.mean(train_metric)\n",
    "            mean_test = np.mean(test_metric)\n",
    "            variance_train = np.var(train_metric)\n",
    "            variance_test = np.var(test_metric)\n",
    "            mean_difference = mean_train - mean_test\n",
    "            \n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Metric': metric_name,\n",
    "                'Mean_Train': mean_train,\n",
    "                'Mean_Test': mean_test,\n",
    "                'Variance_Train': variance_train,\n",
    "                'Variance_Test': variance_test,\n",
    "                'Train-Test_Difference': mean_difference\n",
    "            })\n",
    "        \n",
    "        print(results)\n",
    "    \n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DM2425/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/DM2425/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/DM2425/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/DM2425/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/DM2425/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/DM2425/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Model': 'Logistic Regression', 'Metric': 'accuracy', 'Mean_Train': np.float64(0.8066172087520608), 'Mean_Test': np.float64(0.8066219997465719), 'Variance_Train': np.float64(1.0844921610587807e-07), 'Variance_Test': np.float64(5.268755205832088e-07), 'Train-Test_Difference': np.float64(-4.790994511116864e-06)}, {'Model': 'Logistic Regression', 'Metric': 'precision_macro', 'Mean_Train': np.float64(0.5960936865786358), 'Mean_Test': np.float64(0.5960826080028989), 'Variance_Train': np.float64(3.376579292659026e-08), 'Variance_Test': np.float64(3.3484526617922406e-07), 'Train-Test_Difference': np.float64(1.1078575736900298e-05)}, {'Model': 'Logistic Regression', 'Metric': 'recall_macro', 'Mean_Train': np.float64(0.8591195834395972), 'Mean_Test': np.float64(0.8590653947350285), 'Variance_Train': np.float64(1.108756709818786e-07), 'Variance_Test': np.float64(1.3942206427630029e-06), 'Train-Test_Difference': np.float64(5.4188704568636226e-05)}, {'Model': 'Logistic Regression', 'Metric': 'f1_macro', 'Mean_Train': np.float64(0.6061993522055026), 'Mean_Test': np.float64(0.6061868647810481), 'Variance_Train': np.float64(1.1206452933110475e-07), 'Variance_Test': np.float64(8.964877641618647e-07), 'Train-Test_Difference': np.float64(1.2487424454477036e-05)}]\n",
      "[{'Model': 'Logistic Regression', 'Metric': 'accuracy', 'Mean_Train': np.float64(0.8066172087520608), 'Mean_Test': np.float64(0.8066219997465719), 'Variance_Train': np.float64(1.0844921610587807e-07), 'Variance_Test': np.float64(5.268755205832088e-07), 'Train-Test_Difference': np.float64(-4.790994511116864e-06)}, {'Model': 'Logistic Regression', 'Metric': 'precision_macro', 'Mean_Train': np.float64(0.5960936865786358), 'Mean_Test': np.float64(0.5960826080028989), 'Variance_Train': np.float64(3.376579292659026e-08), 'Variance_Test': np.float64(3.3484526617922406e-07), 'Train-Test_Difference': np.float64(1.1078575736900298e-05)}, {'Model': 'Logistic Regression', 'Metric': 'recall_macro', 'Mean_Train': np.float64(0.8591195834395972), 'Mean_Test': np.float64(0.8590653947350285), 'Variance_Train': np.float64(1.108756709818786e-07), 'Variance_Test': np.float64(1.3942206427630029e-06), 'Train-Test_Difference': np.float64(5.4188704568636226e-05)}, {'Model': 'Logistic Regression', 'Metric': 'f1_macro', 'Mean_Train': np.float64(0.6061993522055026), 'Mean_Test': np.float64(0.6061868647810481), 'Variance_Train': np.float64(1.1206452933110475e-07), 'Variance_Test': np.float64(8.964877641618647e-07), 'Train-Test_Difference': np.float64(1.2487424454477036e-05)}, {'Model': 'Random Forest', 'Metric': 'accuracy', 'Mean_Train': np.float64(1.0), 'Mean_Test': np.float64(0.9666234623533304), 'Variance_Train': np.float64(0.0), 'Variance_Test': np.float64(9.231767537164437e-08), 'Train-Test_Difference': np.float64(0.03337653764666959)}, {'Model': 'Random Forest', 'Metric': 'precision_macro', 'Mean_Train': np.float64(1.0), 'Mean_Test': np.float64(0.943309945389619), 'Variance_Train': np.float64(0.0), 'Variance_Test': np.float64(1.8271260823999194e-06), 'Train-Test_Difference': np.float64(0.05669005461038101)}, {'Model': 'Random Forest', 'Metric': 'recall_macro', 'Mean_Train': np.float64(1.0), 'Mean_Test': np.float64(0.6871628218363692), 'Variance_Train': np.float64(0.0), 'Variance_Test': np.float64(8.79560404697244e-06), 'Train-Test_Difference': np.float64(0.31283717816363077)}, {'Model': 'Random Forest', 'Metric': 'f1_macro', 'Mean_Train': np.float64(1.0), 'Mean_Test': np.float64(0.7582117748405883), 'Variance_Train': np.float64(0.0), 'Variance_Test': np.float64(9.637316126900108e-06), 'Train-Test_Difference': np.float64(0.24178822515941167)}]\n",
      "[{'Model': 'Logistic Regression', 'Metric': 'accuracy', 'Mean_Train': np.float64(0.8066172087520608), 'Mean_Test': np.float64(0.8066219997465719), 'Variance_Train': np.float64(1.0844921610587807e-07), 'Variance_Test': np.float64(5.268755205832088e-07), 'Train-Test_Difference': np.float64(-4.790994511116864e-06)}, {'Model': 'Logistic Regression', 'Metric': 'precision_macro', 'Mean_Train': np.float64(0.5960936865786358), 'Mean_Test': np.float64(0.5960826080028989), 'Variance_Train': np.float64(3.376579292659026e-08), 'Variance_Test': np.float64(3.3484526617922406e-07), 'Train-Test_Difference': np.float64(1.1078575736900298e-05)}, {'Model': 'Logistic Regression', 'Metric': 'recall_macro', 'Mean_Train': np.float64(0.8591195834395972), 'Mean_Test': np.float64(0.8590653947350285), 'Variance_Train': np.float64(1.108756709818786e-07), 'Variance_Test': np.float64(1.3942206427630029e-06), 'Train-Test_Difference': np.float64(5.4188704568636226e-05)}, {'Model': 'Logistic Regression', 'Metric': 'f1_macro', 'Mean_Train': np.float64(0.6061993522055026), 'Mean_Test': np.float64(0.6061868647810481), 'Variance_Train': np.float64(1.1206452933110475e-07), 'Variance_Test': np.float64(8.964877641618647e-07), 'Train-Test_Difference': np.float64(1.2487424454477036e-05)}, {'Model': 'Random Forest', 'Metric': 'accuracy', 'Mean_Train': np.float64(1.0), 'Mean_Test': np.float64(0.9666234623533304), 'Variance_Train': np.float64(0.0), 'Variance_Test': np.float64(9.231767537164437e-08), 'Train-Test_Difference': np.float64(0.03337653764666959)}, {'Model': 'Random Forest', 'Metric': 'precision_macro', 'Mean_Train': np.float64(1.0), 'Mean_Test': np.float64(0.943309945389619), 'Variance_Train': np.float64(0.0), 'Variance_Test': np.float64(1.8271260823999194e-06), 'Train-Test_Difference': np.float64(0.05669005461038101)}, {'Model': 'Random Forest', 'Metric': 'recall_macro', 'Mean_Train': np.float64(1.0), 'Mean_Test': np.float64(0.6871628218363692), 'Variance_Train': np.float64(0.0), 'Variance_Test': np.float64(8.79560404697244e-06), 'Train-Test_Difference': np.float64(0.31283717816363077)}, {'Model': 'Random Forest', 'Metric': 'f1_macro', 'Mean_Train': np.float64(1.0), 'Mean_Test': np.float64(0.7582117748405883), 'Variance_Train': np.float64(0.0), 'Variance_Test': np.float64(9.637316126900108e-06), 'Train-Test_Difference': np.float64(0.24178822515941167)}, {'Model': 'XGBoost', 'Metric': 'accuracy', 'Mean_Train': np.float64(0.9656910140341706), 'Mean_Test': np.float64(0.9553173543281932), 'Variance_Train': np.float64(2.5348955014975403e-08), 'Variance_Test': np.float64(9.369375557524853e-08), 'Train-Test_Difference': np.float64(0.010373659705977367)}, {'Model': 'XGBoost', 'Metric': 'precision_macro', 'Mean_Train': np.float64(0.9283072999661439), 'Mean_Test': np.float64(0.8165526528973622), 'Variance_Train': np.float64(1.231030201549604e-06), 'Variance_Test': np.float64(7.343809807396649e-06), 'Train-Test_Difference': np.float64(0.11175464706878169)}, {'Model': 'XGBoost', 'Metric': 'recall_macro', 'Mean_Train': np.float64(0.6841556759092609), 'Mean_Test': np.float64(0.6141350668170495), 'Variance_Train': np.float64(9.370158230105589e-07), 'Variance_Test': np.float64(7.1774766286771976e-06), 'Train-Test_Difference': np.float64(0.0700206090922113)}, {'Model': 'XGBoost', 'Metric': 'f1_macro', 'Mean_Train': np.float64(0.7527779274121679), 'Mean_Test': np.float64(0.6622382419433628), 'Variance_Train': np.float64(1.301937286267072e-06), 'Variance_Test': np.float64(1.101148928604344e-05), 'Train-Test_Difference': np.float64(0.09053968546880509)}]\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(penalty='l2', C=10, solver='lbfgs', class_weight='balanced', l1_ratio=0.5, max_iter=1000, n_jobs=-1),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', bootstrap=False, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(gamma = 0, learning_rate = 0.3, max_depth = 6, min_child_weight = 1, n_estimators = 200, reg_alpha = 0, reg_lambda = 1,random_state=42, n_jobs=-1)   \n",
    "}\n",
    "performance_results = evaluate_models_with_overfitting_check(X_combined, y_combined, ps, selected_features_RFE_basic, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_AR = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        scale_pos_weight = 20,  \n",
    "        max_delta_step = 5,     \n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
